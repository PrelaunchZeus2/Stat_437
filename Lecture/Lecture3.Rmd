---
title: "Lecture3"
author: "John Salmon"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lecture 3: Clustering

#### Clustering Motivation

- Marketing
- Medicine
- Economics

Clustering is often used in exploratory data analysis or as a starting point of further study.

#### Basic Principles

In general it refers to a broad set of techniques for finding subgroups or clusters in a data set.
- Observations within each group are quite similar to each other
- Observations in different groups are quite different from each other

Clustering depends on the concept of "similarity" (or "Dissimilarity)

2 Popular clustering methods
- k-means clustering
- hierarchical clustering
Specificity in your method is better
Hierarchical clustering is typically more flexible.

function for clustering in R *clusGap* from *cluster* , and *ElemStatLearn* for the data set

Working with Iris data set for examples again. Iris is $\text{dimension } p = 4$

Euclidean distance
$d(x_i, x_j) = (\sum_{k = 1}^{p}(x_{ik}-x_{jk})^2)^\frac{1}{2}$
Distance between two of the same vector = 0
Distance between two opposite vectors $= 2||v||$ if 2 vectors V & W where $W = -V$

3 rules for distance
$d(u,w) + d(w,z) \ge d(u,z)$ Triangular inequality
$d(u,w) = 0 \iff u = w$
$d(u,w) = d(w,u)$

dissimilarity between $x_i \text{and} x_j$ is defined as $d_{ij} = d^2(x_i, x_j)$
smaller $d_{ij}$ means $x_i \text{and} x_j$ are more similar and less dissimilar.

it is much easier to optimize square functions than square root functions because ttaking the derivative of a square root functioin explodes to - as f -> 0.

Using squared euclidean distance is better than using euclideian distance.

```{r}
x1 = c(5.1,3.5,1.3,0.2)
x2 = c(4.9,3.0,1.4,0.2)
x3 = c(4.7,3.2,1.3,0.2)

sqrt(sum((x1-x2)^2))
sqrt(sum((x1-x3)^2))
```

K-means clustering divides observations int K disjoint clusters and assigns each observation to a cluster.
- When $K = 2$ each observations will be assigned to one of 2 disjoint clusters
- Let $k = 1$ or 2 denote the cluster membership of an observation.

Clustering is a mapping C that assigns the ith observation $x_i$ to Cluster $k$
This mapping is many-to-one
it is never many-to-one because the clusters are disjoint

Modern Deep learning classification is one-to-many *BUT* probabilisticlly

*Clustering is just label assignment*

The loss function for K-means seeks to minimize the dissimilarity betweenn points in the same cluster.
In the extreme case where all observations are the same you would hope there would only be 1 cluster.

The number of combinations for N observations disjoint clustering is $N!$(n factorial $n * (n-1) * (n-2) * ... * (n-n + 1))$
Clustering is a combinatorial complexity which is NP hard.

1. Ask the correct questions
2. Find approximate answers
The paradigm has shifted as processing and time is limited.

K-means algorithm
1. Randomly assign a number 1 to k to each observation
2. iterate the following steps
2. (a) for each cluster compute the centroid
2. (b) assign each observation to the cluster whose centroid is closest.

when cluster assignment stops changing we say the algorithm "converges" (perhaps to a local minimum of the loss function)




