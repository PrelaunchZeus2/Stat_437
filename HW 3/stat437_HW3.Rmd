---
title: "Stat 437 HW3"
author: 
     - John Salmon (011745357)
header-includes:
   - \usepackage{bbm}
   - \usepackage{amssymb}
   - \usepackage{amsmath}
   - \usepackage{graphicx,float}
   - \usepackage{natbib}
output:
  pdf_document: default
fontsize: 11pt
---

```{r, echo=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

# General rule

You must complete both Conceptual and Applied exercises. Please show your work and submit your computer codes in order to get points. Providing correct answers without supporting details does not receive full credits. Please upload your answers to the course space. This HW covers

- K-means clustering
- Hierarchical clustering

For an assignment or project, you DO NOT have to submit your answers or reports using typesetting software. However, your answers must be well organized and well legible for grading. Please upload your answers in a document to the course space. Specifically, if you are not able to knit a .Rmd/.rmd file into an output file such as a .pdf, .doc, .docx or .html file that contains your codes, outputs from your codes, your interpretations on the outputs, and your answers in text (possibly with math expressions), please organize your codes, their outputs and your answers in a document in the format given below:

```
Problem or task or question ... 
Codes ...
Outputs ...
Your interpretations ...
```

It is absolutely not OK to just submit your codes only. This will result in a considerable loss of points on your assignments or projects. 


# Conceptual exercises

\noindent
1. Consider the K-means clustering methodology. 
 
1.1) Give a few examples of dissimilarity measures that can be used to measure how dissimilar two observations are. What is the main disadvantage of the squared Euclidean distance as a dissimilarity measure? 

Two common dissimilarity measures are Euclidean Distance between vectors, and the Pearson Correlation Coefficient. The main disadvantage of Squared Euclidean Distance is that as the magnitude of vectors increase, the distance also increases. This is not the case with Pearson Correlation because it is a result of the angle and not the length of the vectors.

1.2) Is it true that standardization of data should be done when features are measured on very different scales? Is it true that employing more features gives more accurate clustering results? Is it true that employing standardized observations gives more accurate clustering results than employing non-standardized ones? Explain each of your answers.

Standardizing data is a decision that should be made based on the domain knowlege of the data. While standardizing can be effective at improving the clustering results (i.e. accuracy) it can also hurt your clustering process by removing the measure of the difference in magnitude between observations.



1.3) Take $K=2$. Provide the loss function that K-means clustering tries to minimize. You need to provide the definition and meaning of each term that appears in the loss function. 

The loss function that K-means tries to minimize is $W(C) = \sum_{k = 1}^{k} \sum_{x_i \text{in cluster k}} d^2(x_i, \bar{x}_k) = N_1 * S^2_1 + ... + N_k * S^2_k$. Where each summand is the inter cluster variability of the assigned observations. In essence, the loss function is measuring the similarity and dissimilarity of the points in each cluster. The k-means algorithm seeks to minimize the variance between each observation in each assigned cluster. 


1.4) What is the "centroid" for a cluster? Is the algorithm, Algorithm 10.1 on page 388 of the Text (which is also provided in the lecture slides), guaranteed to converge to the global minimum of the loss function? Why or why not? What does the argument `nstart` refer to in the command `kmeans`? Why is `nstart` suggested to take a relatively large value? Why do you need to set a random seed by `set.seed()` before you apply `kmeans`?

The centroid of a cluster is the sample mean of all the points assigned to the given cluster.
The algorithm is NOT guaranteed to converge to the global minimum, but will always converge to a local minimum. This is because the algorithm may start "chasing" a local minimum instead of the absolute minimum; in this case it can get trapped. This indicates the importance of having random starting points. The nstart parameter refers to the number of initial random clusters to be tried. Starting with a large value means that there is a larger number of attempts to find the best solution. Finally, the set.seed parameter is important to. ensure that your results are reproducible by others. By setting the seed to a value you make the random number generating algorithms have the same output (depending on operating system).

1.5) Suppose there are 2 underlying clusters but you set the number of clusters to be different than $2$ and apply `kmeans`, will you have good clustering results? Why or why not?

Depending on what you set the value of K (the number of clusters) to and what your standard for good clustering results are you may or may not get acceptable results from the clustering operation. I would say in general that if you were to perform clustering with a large value K on a data set with a small number of true clusters that results would not be as satisfactory. However when examining the results of clustering with 4 clusters on the Iris data set which has 3 true clusters, the performance is fairly decent.

1.6) Is the true number $K_0$ of clusters in data known? When using the command `clusGap` to estimate $K_0$, what does its argument `B` refer to?   

In most real world cases where the dataset is not pre-generated, the true number of clusters is unknown. When using clusGap the `B` argument refers to the number of Monte Carlo Samples to Use for bootstrapping.


\noindent
2. Consider hierarchical clustering.

2.1) What are some advantages of hierarchical clustering over K-means clustering? What is the relationship between the dissimilarity between two clusters and the height of these clusters in the dendrogram that represents a bottom-up tree? 

One Advantage of Hierarchical Clustering over K-means is that you do not have to specify the number of clusters to assign observations to. When examining the dendrogram and the height of the clusters, Observations that fuse at the bottom of the tree are similar to each other, while observations that fuse at the top of the tree end up being very different.

2.2) Explain what it means by saying that "the clusters obtained at different heights from a dendrogram are nested". If a data set has two underlying clustering structures that can be obtained by two different criteria, will these two sets of clusters necessarily be nested? Explain your answer.

2.3) Why is the distance based on Pearson's sample correlation not effected by the magnitude of observations in terms of Euclidean distance? What is the definition of average linkage? Why are average linkage and complete linkage preferred than single linkage in practice?

The distance between the pearson sample correlation is not effected by magnitude because geometrically the Pearson Correlation is $cos(\theta)$ where $\theta$ is the angle between the two given vectors. The length of each vector has no effect on the angle and therefore doesn't effect the calculated metric. 

2.4) What does the command `scale` do? Does `scale` apply row-wise or column-wise? When `scale` is applied to a variable, what will happen to the observations of the variable?

The scale command is the command you use in order to standardize data. By default it will scale observations columnwise if applied to a matrix of observations.

2.5) What is `hclust$height`? How do you find the height at which to cut a dendrogram in order to obtain $5$ clusters?

2.6) When creating a dendrogram, what are some advantages of the command `ggdendrogram{ggdendro}` over the R base command `plot`?

ggdendro will create much more aesthetic plots compared to the base command included in R.

Visualizing clustering results as, e.g., done by Example 1 in "LectureNotes3_notes.pdf".


# Applied exercises

\noindent
3. Please refer to the NYC flight data `nycflights13` that has been discussed in the lecture notes and whose manual can be found at https://cran.r-project.org/web/packages/nycflights13/index.html. We will use `flights`, a tibble from `nycflights13`.

Select from `flights` observations that are for 3 `carrier` "UA", "AA" or "DL", for `month` 7 and 2, and for 4 features `dep_delay`, `arr_delay`, `distance` and `air_time`. Let us try to see if we can use the 4 features to identify if an observation belongs a specific carrier or a specific month. The following tasks and questions are based on the extracted observations. Note that you need to remove `na`'s from the extracted observations.
```{r, select data}
library(dplyr)
library(nycflights13)
flights = flights %>% filter(month %in% c(7, 2), carrier %in% c("UA", "AA", "DL"), distance > 700) #extract observations
flights = flights %>% select(dep_delay, arr_delay, distance, air_time, month, carrier)
flights = flights %>% mutate_at('month', as.factor)
flights = na.omit(flights)
head(flights)
```



3.1) Apply K-means with $K=2$ and $3$ respectively but all with `set.seed(1)` and `nstart=20`. For $K=3$, provide visualization of the clustering results based on true clusters given by `carrier`, whereas for $K=2$, provide visualization of the clustering results based on true clusters given by `month`. Summarize your findings based on the clustering results. You can use the same visualization scheme that is provided by Example 2 in "LectureNotes3_notes.pdf". Try visualization based on different sets of 2 features if your visualization has overlayed points.

```{r, 3.1}
set.seed(1)
#library(ElemStatLearn)
library(cluster)
#library(clusgap)

#perform clustering
k2 = kmeans(flights[,1:4], centers = 2, nstart = 20)
k3 = kmeans(flights[,1:4], centers = 3, nstart = 20)
flights$k2cluster = factor(k2$cluster)
flights$k3cluster = factor(k3$cluster)


library(ggplot2)
k3plot = ggplot(flights, aes(x = air_time, y = arr_delay)) + geom_point(aes(color = k3cluster, shape = carrier)) + theme_bw() + ggtitle("3 Cluster")
k3plot

k2plot = ggplot(flights, aes(x = air_time, y = arr_delay)) + geom_point(aes(color = k2cluster, shape = month)) + theme_bw() + ggtitle("2 Cluster")
k2plot
```
For the $K=3$ clustering, examining the plot and comparing the color of each point to the shape used, we can see that for each of the 3 colors, there are a mix of different shapes included. For example, blue has a mix of triangles(DL), squares(UA), and circles(AA). This mix of shapes and colors appear to be present for each of the 3 clusters. This indicates that the clustering likely was not successful and that based on these features, it is difficult to predict which carrier a given flight is operated by.

For the $K=2$ clustering, it is a very similar situation. There are a mix of the two shapes belonging to each color. Once again indicating that predicting what month a flight occured on based on these 4 features is difficult.


3.2) Use `set.seed(123)` to randomly extract 50 observations, and to these 50 observations, apply hierarchical clustering with average linkage. (i) Cut the dendrogram to obtain 3 clusters with leafs annotated by `carrier` names and resulting clusters colored distinctly, and report the corresponding height of cut. (ii) In addition, cut the dendrogram to obtain 2 clusters with leafs annotated by `month` numbers and resulting clusters colored distinctly, and report the corresponding height of cut. Here are some hints: say, you save the randomly extracted 50 observations into an object `ds3sd`, for these observations save their `carrier` names by keeping their object type but save `month` numbers as a `character` vector, make sure that `ds3sd` is a `matrix`, transpose `ds3sd` into `tmp`, assign to `tmp` column names with their corresponding carrier names or month numbers, and then transpose `tmp` and save it as `ds3sd`; this way, you are done assigning cluster labels to each observation in `ds3sd`; then you are ready to use the commands in the file `Plotggdendro.r` to create the desired dendrograms. 

```{r, 3.2}
set.seed(123)
flightsample = sample(1:dim(flights)[1], size = 50, replace = FALSE)


```


